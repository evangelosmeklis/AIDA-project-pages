{
  
    
        "post0": {
            "title": "Title",
            "content": "&#917;&#965;&#940;&#947;&#947;&#949;&#955;&#959;&#962; &#924;&#949;&#954;&#955;&#942;&#962;, &#913;.&#924;.: 03003065, email: vmeklis@biosim.ntua.gr . &#928;&#945;&#957;&#945;&#947;&#953;&#974;&#964;&#951;&#962; &#935;&#945;&#961;&#945;&#964;&#963;&#940;&#961;&#951;&#962;, &#913;.&#924;.: 03003086, email: pancharatsaris@netmode.ntua.gr . !pip install fiftyone --no-binary fiftyone,voxel51-eta !pip install opencv-python-headless==4.5.4.60 !pip install lime !pip install owlready2 . import fiftyone as fo from skimage import io import torch from torch.autograd import Variable as V import torchvision.models as models from torchvision import transforms as trn from torch.nn import functional as F import os import PIL from urllib.error import HTTPError from owlready2 import * from nltk.corpus import wordnet as wn import nltk import lime from lime import lime_image from skimage.segmentation import mark_boundaries import matplotlib.pyplot as plt import random import numpy as np from numpy import transpose from torchvision import models, transforms import tensorflow as tf from tensorflow import keras from IPython.display import Image, display import matplotlib.cm as cm import os import cv2 import pandas as pd . NumExpr defaulting to 2 threads. Migrating database to v0.16.5 . PIZZERIA . Η μία απο τις δύο κλάσεις που επιλέξαμε ήταν η κλάση pizzeria. Για την κλάση αυτή επιλέξαμε τα υποσύνολα &quot;pizza&quot;, &quot;dining table&quot; και &quot;person&quot; του Coco dataset τα οποία είδαμε ότι έδιναν παρόμοιες εικόνες με αυτές στην κλάση pizzeria. . DATASET . Παρακάτω φαίνεται ο κώδικας που χρησιμοποιήσαμε για να δημιουργήσουμε το dataset για την κλάση pizzeria. Κομμάτια του κώδικα που χρησιμοποιήσαμε πάρθηκαν από τον έτοιμο κώδικα που δόθηκε στα πλαίσια της εργασίας. Αρχικά, γίνεται εισαγωγή του dataset στο notebook με χρήση της βιβλιοθήκης fiftyone. . !rm -rf /root/fiftyone/ . dataset_pizzeria = fo.zoo.load_zoo_dataset( &quot;coco-2017&quot;, split=&quot;train&quot;, classes=[ &quot;pizza&quot;,&quot;dining table&quot;,&quot;person&quot;], max_samples=695, ) . Downloading split &#39;train&#39; to &#39;/root/fiftyone/coco-2017/train&#39; if necessary Downloading annotations to &#39;/root/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip&#39; 100% |██████| 1.9Gb/1.9Gb [3.1s elapsed, 0s remaining, 709.5Mb/s] Extracting annotations to &#39;/root/fiftyone/coco-2017/raw/instances_train2017.json&#39; Downloading 695 images 100% |██████████████████| 695/695 [1.3m elapsed, 0s remaining, 9.3 images/s] Writing annotations for 695 downloaded samples to &#39;/root/fiftyone/coco-2017/train/labels.json&#39; Dataset info written to &#39;/root/fiftyone/coco-2017/info.json&#39; Loading &#39;coco-2017&#39; split &#39;train&#39; 100% |█████████████████| 695/695 [9.8s elapsed, 0s remaining, 75.9 samples/s] Dataset &#39;coco-2017-train-695&#39; created . nltk.download(&#39;wordnet&#39;) nltk.download(&#39;omw-1.4&#39;) . [nltk_data] Downloading package wordnet to /root/nltk_data... [nltk_data] Downloading package omw-1.4 to /root/nltk_data... . True . Χρησιμοποιώντας τον δοσμένο κώδικα αφού έγιναν οι απαραίτητες τροποποιήσεις δημιουργούμε μια οντολογία προσθέτοντας κάθε εικόνα του dataset σε αυτήν. Επιπλέον, κάθε εικόνα αντιστοιχίζεται στα αντικείμενα της καθώς και στις υπερένοιες τους και δημιουργείται ένα αρχείο (myonto_pizzeria.nt) που αναπαριστά όλη αυτή τη γνώση του WordNet. . onto = get_ontology(&#39;http://myontology/&#39;) ids = dataset_pizzeria.values(&quot;id&quot;) with onto: ## Ορισμός εννοιών για εικόνες και αντικείμενα class Image(Thing): namespace=onto pass class DepictedObject(Thing): namespace=onto pass ## Ορισμός ρόλου &quot;hasObject&quot; (ποιες εικόνες περιέχουν ποια αντικείμενα) class hasObject(Image&gt;&gt;DepictedObject): namespace=onto pass ## Για κάθε εικόνα for im_id in ids: ## Δημιουργία individual τύπου &quot;Image&quot; im = onto[&#39;Image&#39;](str(im_id)) ## Για κάθε αντικείμενο στην εικόνα: for temp_obj in dataset_pizzeria[im_id][&#39;ground_truth&#39;][&#39;detections&#39;]: obj_name=temp_obj[&#39;label&#39;] ## Εύρεση πρώτου synset στο wordnet synsets = wn.synsets(obj_name) if len(synsets)&lt;1: continue ## Δημιουργία individual τύπου &quot;Object&quot; obj = onto[&#39;DepictedObject&#39;]() ## Σύνδεση της εικόνας με το αντικείμενο μέσω του ρόλου hasObject im.hasObject.append(obj) synset = wn.synsets(obj_name)[0].name() ## Αν υπάρχει η αντίστοιχη έννοια στη γνώση if onto[synset] in onto.classes(): ## Ορίζουμε το αντικείμενο obj ως τύπου &quot;synset&quot; obj.is_a.append(onto[synset]) ## Αν δεν υπάρχει η αντίστοιχη έννοια στη γνώση, την ορίζουμε, μαζί με τις υπερέννοιές της else: ## Εύρεση υπερώνυμων hyper = lambda s:s.hypernyms() hypers = [s.name() for s in list(wn.synset(synset).closure(hyper))] hypers = reversed(hypers) ## Ορισμός ιεραρχίας εννοιών father = Thing for h in hypers: if onto[h] not in onto.classes(): with onto: cl = types.new_class(h,(father,)) father = onto[h] if onto[synset] not in onto.classes(): with onto: cl = types.new_class(synset,(father,)) ## Ορίζουμε το αντικείμενο obj ως τύπου &quot;synset&quot; with onto: obj.is_a.append(onto[synset]) ## Αποθηκεύουμε την οντολογία onto.save(&#39;myonto_pizzeria.nt&#39;,format=&#39;ntriples&#39;) . /usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset(&#39;physical_entity.n.01&#39;) at depth 5 for synset in acyclic_breadth_first(self, rel, depth): /usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset(&#39;artifact.n.01&#39;) at depth 5 for synset in acyclic_breadth_first(self, rel, depth): /usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset(&#39;physical_entity.n.01&#39;) at depth 9 for synset in acyclic_breadth_first(self, rel, depth): /usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset(&#39;instrumentality.n.03&#39;) at depth 6 for synset in acyclic_breadth_first(self, rel, depth): /usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset(&#39;animal.n.01&#39;) at depth 7 for synset in acyclic_breadth_first(self, rel, depth): /usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset(&#39;instrumentality.n.03&#39;) at depth 4 for synset in acyclic_breadth_first(self, rel, depth): /usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset(&#39;implement.n.01&#39;) at depth 5 for synset in acyclic_breadth_first(self, rel, depth): /usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset(&#39;physical_entity.n.01&#39;) at depth 10 for synset in acyclic_breadth_first(self, rel, depth): /usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset(&#39;artifact.n.01&#39;) at depth 4 for synset in acyclic_breadth_first(self, rel, depth): /usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset(&#39;artifact.n.01&#39;) at depth 7 for synset in acyclic_breadth_first(self, rel, depth): . ENRICHMENT . Σε αυτό το μέρος της εργασίας εφαρόζουμε τον έτοιμο classifier του places στο dataset μας ώστε να εντοπίσουμε την κατηγοριοποίηση που κάνει στις εικόνες που περιέχει. . def read_image_from_url(img_url): img = io.imread(img_url) img = PIL.Image.fromarray(img) return img . class Classifier: def __init__(self, arch): # th architecture to use self.arch = arch # load the pre-trained weights self.model_file = &#39;%s_places365.pth.tar&#39; % self.arch if not os.access(self.model_file, os.W_OK): weight_url = &#39;http://places2.csail.mit.edu/models_places365/&#39; + self.model_file os.system(&#39;wget &#39; + weight_url) self.model = models.__dict__[arch](num_classes=365) checkpoint = torch.load(self.model_file, map_location=lambda storage, loc: storage) state_dict = {str.replace(k,&#39;module.&#39;,&#39;&#39;): v for k,v in checkpoint[&#39;state_dict&#39;].items()} self.model.load_state_dict(state_dict) self.model.eval() # load the image transformer self.centre_crop = trn.Compose([ trn.Resize((256,256)), trn.CenterCrop(224), trn.ToTensor(), trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]) # load the class label file_name = &#39;categories_places365.txt&#39; if not os.access(file_name, os.W_OK): synset_url = &#39;https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt&#39; os.system(&#39;wget &#39; + synset_url) self.classes = list() with open(file_name) as class_file: for line in class_file: self.classes.append(line.strip().split(&#39; &#39;)[0][3:]) self.classes = tuple(self.classes) # load the test image def classify(self, img_url): img = io.imread(img_url) img = PIL.Image.fromarray(img) input_img = V(self.centre_crop(img).unsqueeze(0)) # forward pass logit = self.model.forward(input_img) h_x = F.softmax(logit, 1).data.squeeze() probs, idx = h_x.sort(0, True) # output the prediction preds = [] for i in range(0, 5): preds.append([self.classes[idx[i]], float(probs[i])]) return preds . predictor = Classifier(&quot;resnet18&quot;) . image_url = dataset_pizzeria.values(&quot;filepath&quot;)[0] print (f&quot;Image url: {image_url}&quot;) image = read_image_from_url(image_url) plt.imshow(image) print (&quot;Predictions&quot;) predictor.classify(image_url) . Image url: /root/fiftyone/coco-2017/train/data/000000000110.jpg Predictions . [[&#39;pizzeria&#39;, 0.24589334428310394], [&#39;cafeteria&#39;, 0.23186656832695007], [&#39;food_court&#39;, 0.20682573318481445], [&#39;restaurant&#39;, 0.1310582160949707], [&#39;dining_hall&#39;, 0.05608779564499855]] . Παρατηρούμε πως ο places classifier σύμφωνα με τον κώδικα που χρησιμοποιήσαμε για κάθε εικόνα επιστρέφει πέντε predictions για την κλάση που πιθανώς ανήκουν, καθώς και την πιθανότητα να ανήκει σε αυτή την κλάση. Συγκεκριμένα, για την πρώτη εικόνα του dataset διαπιστώνουμε πως πράγματι την κατατάσει στην κατηγορία pizzeria με την μεγαλύτερη πιθανότητα ενώ και οι επόμενες 4 πιθανές κατηγορίες φαίνεται να είναι σχετικές. . Συνεπώς παρακάτω για κάθε εικόνα που βρίσκεται στο dataset pizzeria που φορτώσαμε από το Coco, χρησιμοποιούμε τον places classifier για να την κατηγοριοποιήσουμε. Μετράμε πόσες φορές κατηγοριοποιήθηκαν ως μία κλάση και κρατάμε τα αποτελέσματα σε ένα dictionary. Επισημάνουμε, πως και οι πέντε κατηγορίες που προκύπτουν απο τον classifier λαμβάνονται υπόψην ενώ παρουσιάζονται αναλυτικά τα αποτέλεσματα των δέκα πρώτων εικόνων του dataset για λόγους εξοικονόμησης χώρου. . classifications = {} print(&quot;Classification Results for 10 first images of pizzeria dataset&quot;) print(&quot;&quot;) count=0 for i in dataset_pizzeria.values(&quot;id&quot;): try: image_url = dataset_pizzeria[i][&quot;filepath&quot;] classif = predictor.classify(image_url) for i in classif: if i[0] in classifications: classifications[i[0]] = classifications[i[0]] + 1 else: classifications[i[0]] = 1 if count&lt;10: print(&quot;The image url is: &quot;, image_url ) print(&quot;Classification: &quot;, classif) print(&quot;&quot;) count+=1 except HTTPError as err: if err.code == 404: print(image_url) print(&quot;Image not found. Continuing...&quot;) print(&quot;&quot;) . Classification Results for 10 first images of pizzeria dataset The image url is: /root/fiftyone/coco-2017/train/data/000000000110.jpg Classification: [[&#39;pizzeria&#39;, 0.24589334428310394], [&#39;cafeteria&#39;, 0.23186656832695007], [&#39;food_court&#39;, 0.20682573318481445], [&#39;restaurant&#39;, 0.1310582160949707], [&#39;dining_hall&#39;, 0.05608779564499855]] The image url is: /root/fiftyone/coco-2017/train/data/000000000397.jpg Classification: [[&#39;pizzeria&#39;, 0.9992170333862305], [&#39;delicatessen&#39;, 0.0002509270270820707], [&#39;bakery/shop&#39;, 0.00022777265985496342], [&#39;restaurant&#39;, 8.09463017503731e-05], [&#39;underwater/ocean_deep&#39;, 3.400118293939158e-05]] The image url is: /root/fiftyone/coco-2017/train/data/000000002742.jpg Classification: [[&#39;pizzeria&#39;, 0.6365488171577454], [&#39;restaurant_kitchen&#39;, 0.26075002551078796], [&#39;delicatessen&#39;, 0.030665883794426918], [&#39;sushi_bar&#39;, 0.01689274236559868], [&#39;bakery/shop&#39;, 0.013873821124434471]] The image url is: /root/fiftyone/coco-2017/train/data/000000003911.jpg Classification: [[&#39;pizzeria&#39;, 0.9351177215576172], [&#39;bakery/shop&#39;, 0.03145100548863411], [&#39;delicatessen&#39;, 0.006833107676357031], [&#39;restaurant&#39;, 0.00658441474661231], [&#39;butchers_shop&#39;, 0.0052942237816751]] The image url is: /root/fiftyone/coco-2017/train/data/000000004312.jpg Classification: [[&#39;restaurant&#39;, 0.37033769488334656], [&#39;pizzeria&#39;, 0.22254414856433868], [&#39;coffee_shop&#39;, 0.18576925992965698], [&#39;cafeteria&#39;, 0.04740091413259506], [&#39;bakery/shop&#39;, 0.04015209153294563]] The image url is: /root/fiftyone/coco-2017/train/data/000000004642.jpg Classification: [[&#39;pizzeria&#39;, 0.9996098875999451], [&#39;bakery/shop&#39;, 0.00021489494247362018], [&#39;delicatessen&#39;, 0.00010433655552333221], [&#39;butchers_shop&#39;, 4.447487663128413e-05], [&#39;restaurant&#39;, 8.202777280530427e-06]] The image url is: /root/fiftyone/coco-2017/train/data/000000006747.jpg Classification: [[&#39;sushi_bar&#39;, 0.14486248791217804], [&#39;recreation_room&#39;, 0.08220162242650986], [&#39;art_school&#39;, 0.0800657719373703], [&#39;science_museum&#39;, 0.06247784569859505], [&#39;restaurant&#39;, 0.05609511584043503]] The image url is: /root/fiftyone/coco-2017/train/data/000000007274.jpg Classification: [[&#39;nursing_home&#39;, 0.8893303871154785], [&#39;dining_room&#39;, 0.06345512717962265], [&#39;dining_hall&#39;, 0.025790847837924957], [&#39;banquet_hall&#39;, 0.004141836892813444], [&#39;restaurant&#39;, 0.0032037210185080767]] The image url is: /root/fiftyone/coco-2017/train/data/000000009002.jpg Classification: [[&#39;pizzeria&#39;, 0.9876405596733093], [&#39;bakery/shop&#39;, 0.005022552330046892], [&#39;butchers_shop&#39;, 0.0037855308037251234], [&#39;fastfood_restaurant&#39;, 0.000820556771941483], [&#39;delicatessen&#39;, 0.0007996802451089025]] The image url is: /root/fiftyone/coco-2017/train/data/000000011198.jpg Classification: [[&#39;pizzeria&#39;, 0.4855290651321411], [&#39;restaurant&#39;, 0.19318409264087677], [&#39;delicatessen&#39;, 0.08555790036916733], [&#39;sushi_bar&#39;, 0.07230882346630096], [&#39;food_court&#39;, 0.030508121475577354]] . Παρακάτω φαίνεται πως στις εικόνες που διατρέξαμε, οι κλάσεις pizzeria - restaurant - bakery/shop - delicatessen ήταν οι κλάσεις που εμφανίστηκαν περισσότερο στην κατηγοριοποίηση των εικόνων. Συνεπώς, πράγματι οι περισσότερες εικόνες κατηγοριοποιήθηκαν στην σωστή κλάση (pizzetia) ενώ και οι κλάσεις που ακολουθούν σε πλήθος αφορούν σχετικά περιβάλλοντα. . pd.set_option(&#39;display.max_rows&#39;, 140) df=pd.DataFrame(classifications.items(), columns=[&#39;Class&#39;, &#39;Number of Images Classified&#39;]) df.loc[(df[&#39;Number of Images Classified&#39;] &gt;= 20)].sort_values(by=[&#39;Number of Images Classified&#39;], ascending=False,ignore_index=True) . Class Number of Images Classified . 0 pizzeria | 543 | . 1 restaurant | 432 | . 2 bakery/shop | 332 | . 3 delicatessen | 307 | . 4 cafeteria | 228 | . 5 restaurant_kitchen | 157 | . 6 coffee_shop | 145 | . 7 ice_cream_parlor | 129 | . 8 butchers_shop | 123 | . 9 food_court | 118 | . 10 dining_hall | 104 | . 11 sushi_bar | 73 | . 12 art_school | 69 | . 13 beer_hall | 56 | . 14 beer_garden | 44 | . 15 market/outdoor | 38 | . 16 kitchen | 30 | . 17 art_studio | 29 | . 18 kindergarden_classroom | 28 | . 19 candy_store | 28 | . 20 restaurant_patio | 22 | . 21 galley | 21 | . 22 pub/indoor | 21 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; EXPLANATION . Για το Βήμα 7 θα εφαρμόσουμε ορισμένες μεθόδους XAI στο dataset μας. Συγκεκριμένα, επιλέχθηκαν το LIME και το GradCAM. . LIME . Παρακάτω φαίνεται ο κώδικας για το LIME. Έγινε χρήση της βιβλιοθήκης lime_image σε συνδιασμό με έναν predictor που βασίστηκε στον classifier του places ου χρησιμοποιήθηκε και παραπάνω. . explainer = lime_image.LimeImageExplainer() . def predict_function(image): preds=[] for i in range(10): img = Image.fromarray(image[i]) input_img = V(predictor.centre_crop(img).unsqueeze(0)) logit = predictor.model.forward(input_img) h_x = F.softmax(logit, 1).data.squeeze() probs, idx = h_x.sort(0, True) probs = probs.numpy() idx = idx.numpy() preds_temp = np.zeros(len(predictor.classes)) for j in range(0, len(preds_temp)): preds_temp[idx[j]]=probs[j] preds.append(preds_temp) return preds . from PIL import Image image = io.imread(dataset_pizzeria.values(&quot;filepath&quot;)[0]) explanation = explainer.explain_instance(image, predict_function, top_labels=5, hide_color=None, num_samples=1000) . from skimage.segmentation import mark_boundaries temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=10, hide_rest=True) img_boundry1 = mark_boundaries(temp/255.0, mask) print(&quot;Image Class: &quot;, predictor.classes[explanation.top_labels[0]]) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,15)) ax1.imshow(img_boundry1) ax2.imshow(image) ax1.axis(&#39;off&#39;) ax2.axis(&#39;off&#39;) . Image Class: pizzeria . (-0.5, 639.5, 479.5, -0.5) . from PIL import Image image = io.imread(dataset_pizzeria.values(&quot;filepath&quot;)[1]) explanation = explainer.explain_instance(image, predict_function, top_labels=5, hide_color=None, num_samples=1000) . from skimage.segmentation import mark_boundaries temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=20, hide_rest=True) img_boundry1 = mark_boundaries(temp/255.0, mask) print(&quot;Image Class: &quot;, predictor.classes[explanation.top_labels[0]]) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,15)) ax1.imshow(img_boundry1) ax2.imshow(image) ax1.axis(&#39;off&#39;) ax2.axis(&#39;off&#39;) . Image Class: pizzeria . (-0.5, 639.5, 479.5, -0.5) . from PIL import Image image = io.imread(dataset_pizzeria.values(&quot;filepath&quot;)[2]) explanation = explainer.explain_instance(image, predict_function, top_labels=5, hide_color=None, num_samples=1000) . from skimage.segmentation import mark_boundaries temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=10, hide_rest=True) img_boundry1 = mark_boundaries(temp/255.0, mask) print(&quot;Image Class: &quot;, predictor.classes[explanation.top_labels[0]]) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,15)) ax1.imshow(img_boundry1) ax2.imshow(image) ax1.axis(&#39;off&#39;) ax2.axis(&#39;off&#39;) . Image Class: pizzeria . (-0.5, 639.5, 479.5, -0.5) . Απο τα παραδείγματα των παραπάνω εικόνων βλέπουμε πως ο classifier εστιάζει περισσότερο στην πίτσα, στα πρόσωπα, στο τραπέζι και τα μαχαιροπίρουνα για να κάνει την κατηγοριοποίηση των εικόνων. . GRADCAM . Αντίστοιχα, δημιουργούμε μια συνάρτηση που εφαρμόζει την μέθοδο του GradCam σε κάθε εικόνα. . def recursion_change_bn(module): if isinstance(module, torch.nn.BatchNorm2d): module.track_running_stats = 1 else: for i, (name, module1) in enumerate(module._modules.items()): module1 = recursion_change_bn(module1) return module def load_labels(): # prepare all the labels # scene category relevant file_name_category = &#39;categories_places365.txt&#39; if not os.access(file_name_category, os.W_OK): synset_url = &#39;https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt&#39; os.system(&#39;wget &#39; + synset_url) classes = list() with open(file_name_category) as class_file: for line in class_file: classes.append(line.strip().split(&#39; &#39;)[0][3:]) classes = tuple(classes) # indoor and outdoor relevant file_name_IO = &#39;IO_places365.txt&#39; if not os.access(file_name_IO, os.W_OK): synset_url = &#39;https://raw.githubusercontent.com/csailvision/places365/master/IO_places365.txt&#39; os.system(&#39;wget &#39; + synset_url) with open(file_name_IO) as f: lines = f.readlines() labels_IO = [] for line in lines: items = line.rstrip().split() labels_IO.append(int(items[-1]) -1) # 0 is indoor, 1 is outdoor labels_IO = np.array(labels_IO) # scene attribute relevant file_name_attribute = &#39;labels_sunattribute.txt&#39; if not os.access(file_name_attribute, os.W_OK): synset_url = &#39;https://raw.githubusercontent.com/csailvision/places365/master/labels_sunattribute.txt&#39; os.system(&#39;wget &#39; + synset_url) with open(file_name_attribute) as f: lines = f.readlines() labels_attribute = [item.rstrip() for item in lines] file_name_W = &#39;W_sceneattribute_wideresnet18.npy&#39; if not os.access(file_name_W, os.W_OK): synset_url = &#39;http://places2.csail.mit.edu/models_places365/W_sceneattribute_wideresnet18.npy&#39; os.system(&#39;wget &#39; + synset_url) W_attribute = np.load(file_name_W) return classes, labels_IO, labels_attribute, W_attribute def hook_feature(module, input, output): features_blobs.append(np.squeeze(output.data.cpu().numpy())) def returnCAM(feature_conv, weight_softmax, class_idx): # generate the class activation maps upsample to 256x256 size_upsample = (256, 256) nc, h, w = feature_conv.shape output_cam = [] for idx in class_idx: cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc, h*w))) cam = cam.reshape(h, w) cam = cam - np.min(cam) cam_img = cam / np.max(cam) cam_img = np.uint8(255 * cam_img) output_cam.append(cv2.resize(cam_img, size_upsample)) return output_cam def returnTF(): # load the image transformer tf = trn.Compose([ trn.Resize((256,256)), trn.CenterCrop(224), trn.ToTensor(), trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]) return tf def load_model(): model_file = &#39;resnet18_places365.pth.tar&#39; if not os.access(model_file, os.W_OK): weight_url = &#39;http://places2.csail.mit.edu/models_places365/&#39; + model_file os.system(&#39;python -m wget &#39; + weight_url) model = models.__dict__[&#39;resnet18&#39;](num_classes=365) checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage) state_dict = {str.replace(k,&#39;module.&#39;,&#39;&#39;): v for k,v in checkpoint[&#39;state_dict&#39;].items()} model.load_state_dict(state_dict) model.eval() # hook the feature extractor features_names = [&#39;layer4&#39;,&#39;avgpool&#39;] # this is the last conv layer of the resnet for name in features_names: model._modules.get(name).register_forward_hook(hook_feature) return model . import IPython.display def xai_gradcam(img_url): # load the labels classes, labels_IO, labels_attribute, W_attribute = load_labels() # load the model global features_blobs features_blobs = [] model = load_model() # load the transformer tf = returnTF() # image transformer # get the softmax weight params = list(model.parameters()) weight_softmax = params[-2].data.numpy() weight_softmax[weight_softmax&lt;0] = 0 # load the test image os.system(&#39;wget %s -q -O test.jpg&#39; % img_url) img = Image.open(img_url) input_img = V(tf(img).unsqueeze(0)) # forward pass logit = model.forward(input_img) h_x = F.softmax(logit, 1).data.squeeze() probs, idx = h_x.sort(0, True) probs = probs.numpy() idx = idx.numpy() print(&#39;RESULT ON &#39; + img_url) # output the IO prediction io_image = np.mean(labels_IO[idx[:10]]) # vote for the indoor or outdoor if io_image &lt; 0.5: print(&#39;--TYPE OF ENVIRONMENT: indoor&#39;) else: print(&#39;--TYPE OF ENVIRONMENT: outdoor&#39;) # output the prediction of scene category print(&#39;--SCENE CATEGORIES:&#39;) for i in range(0, 5): print(&#39;{:.3f} -&gt; {}&#39;.format(probs[i], classes[idx[i]])) # output the scene attributes responses_attribute = W_attribute.dot(features_blobs[1]) idx_a = np.argsort(responses_attribute) print(&#39;--SCENE ATTRIBUTES:&#39;) print(&#39;, &#39;.join([labels_attribute[idx_a[i]] for i in range(-1,-10,-1)])) # generate class activation mapping print(&#39;Class activation map is saved as cam.jpg&#39;) CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]]) # render the CAM and output img = cv2.imread(img_url) height, width, _ = img.shape heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET) result = heatmap * 0.4 + img * 0.5 cv2.imwrite(&#39;cam.jpg&#39;, result) . xai_gradcam(dataset_pizzeria.values(&quot;filepath&quot;)[0]) IPython.display.Image(&#39;cam.jpg&#39;) . RESULT ON /root/fiftyone/coco-2017/train/data/000000000110.jpg --TYPE OF ENVIRONMENT: indoor --SCENE CATEGORIES: 0.376 -&gt; pizzeria 0.077 -&gt; food_court 0.061 -&gt; cafeteria 0.053 -&gt; delicatessen 0.051 -&gt; dining_hall --SCENE ATTRIBUTES: no horizon, man-made, natural light, open area, foliage, vegetation, sunny, enclosed area, trees Class activation map is saved as cam.jpg . xai_gradcam(dataset_pizzeria.values(&quot;filepath&quot;)[1]) IPython.display.Image(&#39;cam.jpg&#39;) . RESULT ON /root/fiftyone/coco-2017/train/data/000000000397.jpg --TYPE OF ENVIRONMENT: indoor --SCENE CATEGORIES: 0.996 -&gt; pizzeria 0.001 -&gt; delicatessen 0.001 -&gt; bakery/shop 0.000 -&gt; underwater/ocean_deep 0.000 -&gt; sushi_bar --SCENE ATTRIBUTES: no horizon, man-made, natural light, open area, trees, wood, enclosed area, foliage, leaves Class activation map is saved as cam.jpg . xai_gradcam(dataset_pizzeria.values(&quot;filepath&quot;)[2]) IPython.display.Image(&#39;cam.jpg&#39;) . RESULT ON /root/fiftyone/coco-2017/train/data/000000002742.jpg --TYPE OF ENVIRONMENT: indoor --SCENE CATEGORIES: 0.679 -&gt; pizzeria 0.147 -&gt; restaurant_kitchen 0.042 -&gt; sushi_bar 0.029 -&gt; delicatessen 0.028 -&gt; galley --SCENE ATTRIBUTES: no horizon, man-made, natural light, open area, sunny, dry, enclosed area, cloth, warm Class activation map is saved as cam.jpg . Το GradCAM όπως παρατηρούμε παράγει ένα heatmap για κάθε εικόνα. Από τις θερμότερες περιοχές παρατηρούμε πως ο classifier βασίζεται στην πίτσα, στο τραπέζι και τα μαχαιροπίρουνα για να κάνει την κατηγοριοποίηση. . Συνεπώς το LIME και το GradCAM συμφωνούν ως προς τα αντικείμενα που συμβάλλουν περισσότερο στις αποφάσεις του classifier με μικρές διαφοροποιήσεις στα σημεία. Η βασική διαφορά των δύο αυτών μεθόδων είναι πως ενώ με το LIME καταφέρνουμε να εντοπίσουμε τα σημαντικότερα μόνο σημεία για την κατηγοριοποίηση της κάθε εικόνας, με το GradCAM και το heatmap που παράγει, φαίνονται όχι μόνο τα σημεία που δίνεται έμφαση απο τον classifier αλλά το πόσο συμβάλει κάθε pixel της εικόνας. Επιπλέον, εύκολα μπορούμε να διαπιστώσουμε πως ο χρόνος εκτέλεσης για την παραγωγή του heatmap είναι σημαντικά μικρότερος του αντίστοιχου της μεθόδου του LIME. . AIRFIELD . Παρακάτω εφαρμόζουμε την ίδια διαδικασία σε όλα τα βήματα για την κλάση airfield. Οι κλάσεις του Coco που επιλέξαμε είναι το airplane, truck και person. . DATASET . !rm -rf /root/fiftyone/ . dataset_airfield = fo.zoo.load_zoo_dataset( &quot;coco-2017&quot;, split=&quot;train&quot;, classes=[ &quot;airplane&quot;,&quot;truck&quot;,&quot;person&quot;], max_samples=500, ) . Downloading split &#39;train&#39; to &#39;/root/fiftyone/coco-2017/train&#39; if necessary Downloading annotations to &#39;/root/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip&#39; 100% |██████| 1.9Gb/1.9Gb [2.9s elapsed, 0s remaining, 735.6Mb/s] Extracting annotations to &#39;/root/fiftyone/coco-2017/raw/instances_train2017.json&#39; Downloading 500 images 100% |██████████████████| 500/500 [56.9s elapsed, 0s remaining, 9.0 images/s] Writing annotations for 500 downloaded samples to &#39;/root/fiftyone/coco-2017/train/labels.json&#39; Dataset info written to &#39;/root/fiftyone/coco-2017/info.json&#39; Loading &#39;coco-2017&#39; split &#39;train&#39; 100% |█████████████████| 500/500 [5.1s elapsed, 0s remaining, 106.4 samples/s] Dataset &#39;coco-2017-train-500&#39; created . onto = get_ontology(&#39;http://myontology/&#39;) ## ids των 100 πρώτων εικόνων ids = dataset_airfield.values(&quot;id&quot;) with onto: ## Ορισμός εννοιών για εικόνες και αντικείμενα class Image(Thing): namespace=onto pass class DepictedObject(Thing): namespace=onto pass ## Ορισμός ρόλου &quot;hasObject&quot; (ποιες εικόνες περιέχουν ποια αντικείμενα) class hasObject(Image&gt;&gt;DepictedObject): namespace=onto pass ## Για κάθε εικόνα for im_id in ids: ## Δημιουργία individual τύπου &quot;Image&quot; im = onto[&#39;Image&#39;](str(im_id)) ## Για κάθε αντικείμενο στην εικόνα: for temp_obj in dataset_airfield[im_id][&#39;ground_truth&#39;][&#39;detections&#39;]: obj_name=temp_obj[&#39;label&#39;] ## Εύρεση πρώτου synset στο wordnet synsets = wn.synsets(obj_name) if len(synsets)&lt;1: continue ## Δημιουργία individual τύπου &quot;Object&quot; obj = onto[&#39;DepictedObject&#39;]() ## Σύνδεση της εικόνας με το αντικείμενο μέσω του ρόλου hasObject im.hasObject.append(obj) synset = wn.synsets(obj_name)[0].name() ## Αν υπάρχει η αντίστοιχη έννοια στη γνώση if onto[synset] in onto.classes(): ## Ορίζουμε το αντικείμενο obj ως τύπου &quot;synset&quot; obj.is_a.append(onto[synset]) ## Αν δεν υπάρχει η αντίστοιχη έννοια στη γνώση, την ορίζουμε, μαζί με τις υπερέννοιές της else: ## Εύρεση υπερώνυμων hyper = lambda s:s.hypernyms() hypers = [s.name() for s in list(wn.synset(synset).closure(hyper))] hypers = reversed(hypers) ## Ορισμός ιεραρχίας εννοιών father = Thing for h in hypers: if onto[h] not in onto.classes(): with onto: cl = types.new_class(h,(father,)) father = onto[h] if onto[synset] not in onto.classes(): with onto: cl = types.new_class(synset,(father,)) ## Ορίζουμε το αντικείμενο obj ως τύπου &quot;synset&quot; with onto: obj.is_a.append(onto[synset]) ## Αποθηκεύουμε την οντολογία onto.save(&#39;myonto_airfield.nt&#39;,format=&#39;ntriples&#39;) . /usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset(&#39;artifact.n.01&#39;) at depth 4 for synset in acyclic_breadth_first(self, rel, depth): /usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset(&#39;instrumentality.n.03&#39;) at depth 4 for synset in acyclic_breadth_first(self, rel, depth): . ENRICHMENT . image_url = dataset_airfield.values(&quot;filepath&quot;)[0] print (f&quot;Image url: {image_url}&quot;) image = read_image_from_url(image_url) plt.imshow(image) print (&quot;Predictions&quot;) predictor.classify(image_url) . Image url: /root/fiftyone/coco-2017/train/data/000000002258.jpg Predictions . [[&#39;airfield&#39;, 0.774935781955719], [&#39;runway&#39;, 0.18729139864444733], [&#39;landing_deck&#39;, 0.022925863042473793], [&#39;heliport&#39;, 0.014005308039486408], [&#39;hangar/outdoor&#39;, 0.00042196278809569776]] . Παρατηρούμε πως ο places classifier την πρώτη εικόνα του dataset την κατατάσει στην κατηγορία airfield με πολύ μεγάλη πιθανότητα ενώ και οι επόμενες 4 πιθανές κατηγορίες είναι σχετικές με αεροδρόμια και αεροπλάνα. . Στην συνέχεια παρουσιάζονται αναλυτικά και πάλι τα αποτέλεσματα των δέκα πρώτων μόνο εικόνων του dataset για λόγους εξοικονόμησης χώρου. . classifications = {} print(&quot;Classification Results for 10 first images of airfield dataset&quot;) print(&quot;&quot;) count=0 for i in dataset_airfield.values(&quot;id&quot;): try: image_url = dataset_airfield[i][&quot;filepath&quot;] classif = predictor.classify(image_url) for i in classif: if i[0] in classifications: classifications[i[0]] = classifications[i[0]] + 1 else: classifications[i[0]] = 1 if count&lt;10: print(&quot;The image url is: &quot;, image_url ) print(&quot;Classification: &quot;, classif) print(&quot;&quot;) count+=1 except HTTPError as err: if err.code == 404: print(image_url) print(&quot;Image not found. Continuing...&quot;) print(&quot;&quot;) except: print(image_url) print(&quot;Image not found. Continuing...&quot;) print(&quot;&quot;) . Classification Results for 10 first images of airfield dataset The image url is: /root/fiftyone/coco-2017/train/data/000000002258.jpg Classification: [[&#39;airfield&#39;, 0.774935781955719], [&#39;runway&#39;, 0.18729139864444733], [&#39;landing_deck&#39;, 0.022925863042473793], [&#39;heliport&#39;, 0.014005308039486408], [&#39;hangar/outdoor&#39;, 0.00042196278809569776]] The image url is: /root/fiftyone/coco-2017/train/data/000000004891.jpg Classification: [[&#39;runway&#39;, 0.7311585545539856], [&#39;airfield&#39;, 0.15963232517242432], [&#39;landing_deck&#39;, 0.08081459999084473], [&#39;heliport&#39;, 0.023257963359355927], [&#39;hangar/indoor&#39;, 0.0017733422573655844]] The image url is: /root/fiftyone/coco-2017/train/data/000000005482.jpg Classification: [[&#39;hangar/indoor&#39;, 0.22846141457557678], [&#39;arena/performance&#39;, 0.17744070291519165], [&#39;science_museum&#39;, 0.06969361007213593], [&#39;stage/indoor&#39;, 0.05573877692222595], [&#39;orchestra_pit&#39;, 0.053251609206199646]] The image url is: /root/fiftyone/coco-2017/train/data/000000006562.jpg Classification: [[&#39;baseball_field&#39;, 0.5795655846595764], [&#39;volleyball_court/outdoor&#39;, 0.2627154588699341], [&#39;kennel/outdoor&#39;, 0.038084037601947784], [&#39;soccer_field&#39;, 0.027144720777869225], [&#39;stadium/baseball&#39;, 0.019871780648827553]] The image url is: /root/fiftyone/coco-2017/train/data/000000006901.jpg Classification: [[&#39;parking_lot&#39;, 0.3244086802005768], [&#39;boat_deck&#39;, 0.20181286334991455], [&#39;assembly_line&#39;, 0.07135102897882462], [&#39;landing_deck&#39;, 0.058022771030664444], [&#39;auto_factory&#39;, 0.041022397577762604]] The image url is: /root/fiftyone/coco-2017/train/data/000000009138.jpg Classification: [[&#39;landing_deck&#39;, 0.5236601829528809], [&#39;runway&#39;, 0.45957106351852417], [&#39;airfield&#39;, 0.00934750959277153], [&#39;heliport&#39;, 0.005774491000920534], [&#39;hangar/indoor&#39;, 0.0005707273376174271]] The image url is: /root/fiftyone/coco-2017/train/data/000000010342.jpg Classification: [[&#39;airport_terminal&#39;, 0.2663465738296509], [&#39;train_interior&#39;, 0.14549720287322998], [&#39;runway&#39;, 0.13740235567092896], [&#39;airplane_cabin&#39;, 0.08861993998289108], [&#39;bus_interior&#39;, 0.03473982959985733]] The image url is: /root/fiftyone/coco-2017/train/data/000000011877.jpg Classification: [[&#39;runway&#39;, 0.6998512148857117], [&#39;airfield&#39;, 0.19460558891296387], [&#39;landing_deck&#39;, 0.07630885392427444], [&#39;hangar/outdoor&#39;, 0.022056780755519867], [&#39;heliport&#39;, 0.004609378520399332]] The image url is: /root/fiftyone/coco-2017/train/data/000000016418.jpg Classification: [[&#39;runway&#39;, 0.6536350846290588], [&#39;landing_deck&#39;, 0.28610455989837646], [&#39;airfield&#39;, 0.046636562794446945], [&#39;hangar/outdoor&#39;, 0.0071989260613918304], [&#39;heliport&#39;, 0.003645900869742036]] The image url is: /root/fiftyone/coco-2017/train/data/000000017675.jpg Classification: [[&#39;runway&#39;, 0.7439258694648743], [&#39;airfield&#39;, 0.19460582733154297], [&#39;landing_deck&#39;, 0.05912747606635094], [&#39;heliport&#39;, 0.0013201108667999506], [&#39;hangar/outdoor&#39;, 0.0006674256292171776]] /root/fiftyone/coco-2017/train/data/000000249835.jpg Image not found. Continuing... /root/fiftyone/coco-2017/train/data/000000385625.jpg Image not found. Continuing... /root/fiftyone/coco-2017/train/data/000000537427.jpg Image not found. Continuing... /root/fiftyone/coco-2017/train/data/000000000086.jpg Image not found. Continuing... /root/fiftyone/coco-2017/train/data/000000000821.jpg Image not found. Continuing... . Παρακάτω φαίνεται πως στις εικόνες που διατρέξαμε, οι κλάσεις runway - airfield - helicopter - landing deck ήταν οι κλάσεις που εμφανίστηκαν περισσότερο στην κατηγοριοποίηση των εικόνων με πολύ κοντινά πλήθη. Σε αυτήν την περίπτωση η κατηγοριοποίηση στη επιθυμητή κλάση airfield δεν υπερισχύει σημαντικά σε σχέση με τις υπόλοιπες, όπως συνέβει στην περίπτωση της προηγούμενης κατηγορίας (pizzeria). Αυτο συμβαίνει διότι οι κλάσεις που αφορούν το dataset που μελετάμε είναι πολύ σχετικές μεταξύς τους και εύκολα μπορεί να υπάρξει ενναλαγή. . pd.set_option(&#39;display.max_rows&#39;, 140) df=pd.DataFrame(classifications.items(), columns=[&#39;Class&#39;, &#39;Number of Images Classified&#39;]) df.loc[(df[&#39;Number of Images Classified&#39;] &gt;= 20)].sort_values(by=[&#39;Number of Images Classified&#39;], ascending=False,ignore_index=True) . Class Number of Images Classified . 0 runway | 328 | . 1 airfield | 303 | . 2 heliport | 300 | . 3 landing_deck | 276 | . 4 hangar/outdoor | 232 | . 5 hangar/indoor | 76 | . 6 parking_lot | 27 | . 7 raceway | 27 | . 8 army_base | 20 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; EXPLANATION . LIME . explainer = lime_image.LimeImageExplainer() . def predict_function(image): preds=[] for i in range(10): img = Image.fromarray(image[i]) input_img = V(predictor.centre_crop(img).unsqueeze(0)) logit = predictor.model.forward(input_img) h_x = F.softmax(logit, 1).data.squeeze() probs, idx = h_x.sort(0, True) probs = probs.numpy() idx = idx.numpy() preds_temp = np.zeros(len(predictor.classes)) for j in range(0, len(preds_temp)): preds_temp[idx[j]]=probs[j] preds.append(preds_temp) return preds . from PIL import Image image = io.imread(dataset_airfield.values(&quot;filepath&quot;)[0]) explanation = explainer.explain_instance(image, predict_function, top_labels=5, hide_color=None, num_samples=1000) . from skimage.segmentation import mark_boundaries temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=15, hide_rest=True) img_boundry1 = mark_boundaries(temp/255.0, mask) print(&quot;Image Class: &quot;, predictor.classes[explanation.top_labels[0]]) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,15)) ax1.imshow(img_boundry1) ax2.imshow(image) ax1.axis(&#39;off&#39;) ax2.axis(&#39;off&#39;) . Image Class: airfield . (-0.5, 639.5, 479.5, -0.5) . from PIL import Image image = io.imread(dataset_airfield.values(&quot;filepath&quot;)[11]) explanation = explainer.explain_instance(image, predict_function, top_labels=5, hide_color=None, num_samples=1000) . from skimage.segmentation import mark_boundaries temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=20, hide_rest=True) img_boundry1 = mark_boundaries(temp/255.0, mask) print(&quot;Image Class: &quot;, predictor.classes[explanation.top_labels[0]]) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,15)) ax1.imshow(img_boundry1) ax2.imshow(image) ax1.axis(&#39;off&#39;) ax2.axis(&#39;off&#39;) . Image Class: airfield . (-0.5, 639.5, 431.5, -0.5) . from PIL import Image image = io.imread(dataset_airfield.values(&quot;filepath&quot;)[15]) explanation = explainer.explain_instance(image, predict_function, top_labels=5, hide_color=None, num_samples=1000) . from skimage.segmentation import mark_boundaries temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=20, hide_rest=True) img_boundry1 = mark_boundaries(temp/255.0, mask) print(&quot;Image Class: &quot;, predictor.classes[explanation.top_labels[0]]) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,15)) ax1.imshow(img_boundry1) ax2.imshow(image) ax1.axis(&#39;off&#39;) ax2.axis(&#39;off&#39;) . Image Class: airfield . (-0.5, 639.5, 287.5, -0.5) . Aπό τον αλγόριθμο LIME βλέπουμε πως ο classifier εστιάζει περισσότερο στο αεροπλάνο, τον αεροδιάδρομο και τα οχήματα που βρίσκονται σε αυτόν για να κάνει την κατηγοριοποιήση του. . GRADCAM . xai_gradcam(dataset_airfield.values(&quot;filepath&quot;)[0]) IPython.display.Image(&#39;cam.jpg&#39;) . RESULT ON /root/fiftyone/coco-2017/train/data/000000002258.jpg --TYPE OF ENVIRONMENT: outdoor --SCENE CATEGORIES: 0.777 -&gt; airfield 0.128 -&gt; runway 0.047 -&gt; landing_deck 0.044 -&gt; heliport 0.002 -&gt; hangar/outdoor --SCENE ATTRIBUTES: man-made, no horizon, natural light, enclosed area, cloth, open area, sunny, dry, asphalt Class activation map is saved as cam.jpg . xai_gradcam(dataset_airfield.values(&quot;filepath&quot;)[11]) IPython.display.Image(&#39;cam.jpg&#39;) . RESULT ON /root/fiftyone/coco-2017/train/data/000000024585.jpg --TYPE OF ENVIRONMENT: outdoor --SCENE CATEGORIES: 0.568 -&gt; airfield 0.157 -&gt; heliport 0.100 -&gt; landing_deck 0.086 -&gt; runway 0.050 -&gt; hangar/outdoor --SCENE ATTRIBUTES: man-made, no horizon, natural light, open area, sunny, enclosed area, cloth, dry, driving Class activation map is saved as cam.jpg . xai_gradcam(dataset_airfield.values(&quot;filepath&quot;)[15]) IPython.display.Image(&#39;cam.jpg&#39;) . RESULT ON /root/fiftyone/coco-2017/train/data/000000040103.jpg --TYPE OF ENVIRONMENT: outdoor --SCENE CATEGORIES: 0.747 -&gt; airfield 0.088 -&gt; runway 0.066 -&gt; heliport 0.037 -&gt; hangar/outdoor 0.034 -&gt; landing_deck --SCENE ATTRIBUTES: man-made, no horizon, natural light, open area, enclosed area, cloth, sunny, wood, dry Class activation map is saved as cam.jpg . Τα αποτελέσματα του LIME επιβεβαιώνει και το GradCAM όπου στα heatmaps βλέπουμε πως το αεροπλάνο παίζει τον μεγαλύτερο ρόλο στην κατηγοριοποίηση και λίγο λιγότερο ο αεροδιάδρομος, τα υπόλοιπα οχήματα και οι άνθρωποι. . Ως προς τις διαφορές των δύο μεθόδων μπορούμε να κάνουμε τις ίδιες ακριβώς παρατηρήσεις με την προηγούμενη περίπτωση της κλάσης pizzeria. . ΟΝΤΟΛΟΓΙΕΣ ΚΑΙ GRAPHDB . Στο τελευταίο μέρος της εργασίας θα χρησιμοποιήσουμε τα αποτελέσματα που προέκυψαν απο τις μεθόδους XAI για τα dataset των δύο κατηγοριών και κάνοντας χρήση του graphDB θα τα συγκρινουμε με τα αντίστοιχα που προέκυψαν απο τον έτοιμο classifier του places. . Στο graphDB για την πρώτη κατηγορία αφού φορτώσαμε το αντίστοιχο αρχείο οντολογίας που δημιουργήσαμε παραπάνω σε ένα repository, χρησιμοποιήσαμε το παρακάτω query για τη κλάση pizzeria. Για την δημιουργία του query εστιάσαμε στα αντίκειμενα που οι μέθοδοι XAI υπέδειξαν ως τα σημαντικότερα για την κατηγοριοποίηση των εικόνων του classifier. Μετά απο δοκιμές, ώστε να επιτευχθούν τα καλύτερα δυνατά αποτελέσματα στο query συμπεριλήφθηκαν οι εξής 4 συνδιασμοί αντικειμένων που πρέπει να περιέχονται στην εικόνα για να καταταχθεί στην κατηγορία pizzeria: . Πίτσα - Καρέκλα | Πίτσα - Μαχαίρι - Κούπα | Άνθρωπος - Πίτσα - Κούπα | Άνθρωπος - Πίτσα - Μπουκάλι | Συγκεκριμένα ο classifier κατηγοριοποίησε 543 εικόνες στην κλάση pizzeria. Ενώ το παρακάτω query δίνει 553 αποτελέσματα. Συνεπώς, βλέπουμε πως τα αποτελέσματα που δίνουν τα αξιώματα είναι κοντινά με αυτά του classifier και άρα επαληθεύονται και τα αποτελέσματα των μέθοδων XAI. . prefix my:&lt;http://myontology/&gt; select distinct ?image where { { ?image my:hasObject ?o . ?o a my:pizza.n.01. ?image my:hasObject ?z . ?z a my:chair.n.01. } UNION { ?image my:hasObject ?o . ?o a my:pizza.n.01. ?image my:hasObject ?t . ?t a my:knife.n.01. ?image my:hasObject ?t . ?t a my:cup.n.01. } UNION { ?image my:hasObject ?t . ?t a my:person.n.01. ?image my:hasObject ?o . ?o a my:pizza.n.01. ?image my:hasObject ?z . ?z a my:cup.n.01. } UNION { ?image my:hasObject ?t . ?t a my:person.n.01. ?image my:hasObject ?o . ?o a my:pizza.n.01. ?image my:hasObject ?z . ?z a my:bottle.n.01. } } . Αντίστοιχα χρησιμοποιήσαμε το παρακάτω query με την οντολογία που παραξάμε για τη κλάση airfield. Η λογική για την διαμόρφωση του query ήταν όμοια με αυτή για το pizzeria ενώ οι 4 συνδιασμοί αντικειμένων που προέκυψαν είναι οι εξής: . Αεροπλάνο - Αυτοκίνητο | Άνθρωπος - Τσάντα - Αεροπλάνο | Λεωφορείο - Αεροπλάνο - Σακίδιο | Φορτηγό - Αεροπλάνο | Ο places classifier κατηγοριοποίησε 303 εικόνες ως airfield, ενώ το graphDB με τα αξιώματα έδωσε 363 αποτελέσματα. Επόμενως παρατηρούμε πως και εδώ τα αποτελέσματα είναι κοντινά. . prefix my:&lt;http://myontology/&gt; select distinct ?image where { { ?image my:hasObject ?o . ?o a my:airplane.n.01. ?image my:hasObject ?z . ?z a my:car.n.01. } UNION { ?image my:hasObject ?o . ?o a my:person.n.01. ?image my:hasObject ?t . ?t a my:bag.n.06. ?image my:hasObject ?z . ?z a my:airplane.n.01. } UNION { ?image my:hasObject ?t . ?t a my:bus.n.01. ?image my:hasObject ?o . ?o a my:airplane.n.01. ?image my:hasObject ?z . ?z a my:backpack.n.01. } UNION { ?image my:hasObject ?t . ?t a my:truck.n.01. ?image my:hasObject ?o . ?o a my:airplane.n.01. } } .",
            "url": "https://evangelosmeklis.github.io/AIDA-project-pages/2022/07/12/AIDA-project.html",
            "relUrl": "/2022/07/12/AIDA-project.html",
            "date": " • Jul 12, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://evangelosmeklis.github.io/AIDA-project-pages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://evangelosmeklis.github.io/AIDA-project-pages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://evangelosmeklis.github.io/AIDA-project-pages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://evangelosmeklis.github.io/AIDA-project-pages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}